{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIldJyhfI3Hx",
        "outputId": "64cebc4c-1621-4995-b85a-a13bc1ca8450"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-19 22:40:12.764590: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-19 22:40:13.472975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/home/csgrad/sougatas/environments/dev/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "2023-08-19 22:40:19,092\tINFO util.py:159 -- Outdated packages:\n",
            "  ipywidgets==7.6.5 found, needs ipywidgets>=8\n",
            "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
            "2023-08-19 22:40:19,145\tINFO util.py:159 -- Outdated packages:\n",
            "  ipywidgets==7.6.5 found, needs ipywidgets>=8\n",
            "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import tweetnlp\n",
        "from inference.engine import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anM_cAl_6qfD"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "\n",
        "def remove_emoji(text):\n",
        "    return emoji.get_emoji_regexp().sub(u'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAqEgY-V6qfE",
        "outputId": "e9745dbd-2588-44e5-cfbb-15b4bd75c263"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['train_A_AH_HASOC2023.csv',\n",
              " 'train_BE_AH_HASOC2023.csv',\n",
              " 'train_BO_AH_HASOC2023.csv']"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[i for i in os.listdir(task4_path) if i.endswith(\".csv\") and i.startswith(\"train\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IqxGqAp6qfE"
      },
      "outputs": [],
      "source": [
        "lang_map = {'A': {'src':'asm_Beng', 'dest': ['ben_Beng', 'brx_Deva']},\n",
        "            'BE': {'src':'ben_Beng', 'dest': ['asm_Beng', 'brx_Deva']},\n",
        "            'BO': {'src':'brx_Deva', 'dest': ['asm_Beng', 'ben_Beng']}\n",
        "           }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69YBtUJf6qfE"
      },
      "source": [
        "## Translations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We translate the data for each language to the other languages and add to their training set"
      ],
      "metadata": {
        "id": "L0TIyujm664d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC5tgoaU6qfF",
        "outputId": "287ae409-22cc-4d2a-8492-60031b30b2d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing sentencepiece model for SRC and TGT\n",
            "Initializing model for translation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-19 14:56:12 | INFO | fairseq.tasks.translation | [SRC] dictionary: 122706 types\n",
            "2023-08-19 14:56:12 | INFO | fairseq.tasks.translation | [TGT] dictionary: 32296 types\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing sentencepiece model for SRC and TGT\n",
            "Initializing model for translation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-19 14:56:31 | INFO | fairseq.tasks.translation | [SRC] dictionary: 32322 types\n",
            "2023-08-19 14:56:31 | INFO | fairseq.tasks.translation | [TGT] dictionary: 122672 types\n"
          ]
        }
      ],
      "source": [
        "from inference.engine import Model\n",
        "\n",
        "ind_en_model = Model(\"~/IndicTrans2/indic-en-preprint/fairseq_model/\",\n",
        "                     model_type=\"fairseq\")\n",
        "en_ind_model = Model(\"~/IndicTrans2/en-indic-preprint/fairseq_model/\",\n",
        "                     model_type=\"fairseq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axstyhwX6qfJ"
      },
      "outputs": [],
      "source": [
        "import demoji\n",
        "# demoji.findall(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG--hSD7JIxv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "task4_path = \"~/hasoc_2023_ub/datasets/task4_annihilate/\"\n",
        "\n",
        "data_df, trans_df = pd.DataFrame(), pd.DataFrame()\n",
        "\"\"\" Indic to English \"\"\"\n",
        "for file in tqdm([i for i in os.listdir(task4_path) if i.endswith(\".csv\") and i.startswith(\"train\")]):\n",
        "    print(\"PROCESSING FILE:\",file)\n",
        "    tmpdf = pd.read_csv(task4_path+file)\n",
        "    split, lang, _, _ = file.split(\"/\")[-1].split(\"_\")\n",
        "    tmpdf[\"split\"], tmpdf[\"lang\"] = split, lang\n",
        "    tmpdf[\"truncated_text\"] = tmpdf[\"text\"].apply(lambda x : \" \".join(x.split(\" \")[:50]))\n",
        "    data_df = pd.concat([data_df, tmpdf]).reset_index(drop=True)\n",
        "\n",
        "    \"\"\" TRANSLATIONS \"\"\"\n",
        "    sents, labels = list(tmpdf[\"truncated_text\"]), list(tmpdf[\"task_1\"])\n",
        "\n",
        "    eng_trans = [ind_en_model.translate_paragraph(i, lang_map[lang][\"src\"], \"eng_Latn\") for i in tqdm(sents)]\n",
        "    tdf = pd.DataFrame(sents, columns=[\"src_text\"])\n",
        "    tdf[\"translated_text\"], tdf[\"src_lang\"], tdf[\"dest_lang\"], tdf[\"label\"] = eng_trans, lang_map[lang][\"src\"], \\\n",
        "                                                                                            \"eng_Latn\", labels\n",
        "\n",
        "    assert len(sents) == len(labels) == len(eng_trans)\n",
        "    trans_df = pd.concat([trans_df, tdf]).reset_index(drop=True)\n",
        "\n",
        "    \"\"\" English to Indic \"\"\"\n",
        "    for dest in lang_map[lang][\"dest\"]:\n",
        "        trans = [en_ind_model.translate_paragraph(i, \"eng_Latn\", dest) for i in tqdm(eng_trans)]\n",
        "        assert len(trans) == len(labels) == len(eng_trans)\n",
        "        tdf = pd.DataFrame(eng_trans, columns=[\"src_text\"])\n",
        "        tdf[\"translated_text\"], tdf[\"src_lang\"], tdf[\"dest_lang\"], tdf[\"label\"] = trans, \"eng_Latn\", dest, labels\n",
        "        trans_df = pd.concat([trans_df, tdf]).reset_index(drop=True)\n",
        "    print(\"DONE PROCESSING FILE:\",file,\"\\n\",\"=====\"*5,\"\\n\")\n",
        "data_df.shape, trans_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrmqEaej6qfL",
        "outputId": "d3424dac-7286-4a72-aa1d-fc0d88336650"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S. No.</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>split</th>\n",
              "      <th>lang</th>\n",
              "      <th>truncated_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>লাগিলে ইযাৰ পৰাই কলি লাগাম</td>\n",
              "      <td>NOT</td>\n",
              "      <td>train</td>\n",
              "      <td>A</td>\n",
              "      <td>লাগিলে ইযাৰ পৰাই কলি লাগাম</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   S. No.                        text task_1  split lang  \\\n",
              "0       1  লাগিলে ইযাৰ পৰাই কলি লাগাম    NOT  train    A   \n",
              "\n",
              "               truncated_text  \n",
              "0  লাগিলে ইযাৰ পৰাই কলি লাগাম  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAiaQ_QM6qfL",
        "outputId": "84200a64-dcf1-4916-b24a-2410ded4b61c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src_text</th>\n",
              "      <th>translated_text</th>\n",
              "      <th>src_lang</th>\n",
              "      <th>dest_lang</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>লাগিলে ইযাৰ পৰাই কলি লাগাম</td>\n",
              "      <td>If only I could get a call from him</td>\n",
              "      <td>asm_Beng</td>\n",
              "      <td>eng_Latn</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     src_text                      translated_text  src_lang  \\\n",
              "0  লাগিলে ইযাৰ পৰাই কলি লাগাম  If only I could get a call from him  asm_Beng   \n",
              "\n",
              "  dest_lang label  \n",
              "0  eng_Latn   NOT  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trans_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr3JmFQ06qfM",
        "outputId": "15faea31-6bef-4936-c945-50a671e0721a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>src_text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>src_lang</th>\n",
              "      <th>dest_lang</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>asm_Beng</th>\n",
              "      <th>eng_Latn</th>\n",
              "      <td>4036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ben_Beng</th>\n",
              "      <th>eng_Latn</th>\n",
              "      <td>1281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brx_Deva</th>\n",
              "      <th>eng_Latn</th>\n",
              "      <td>1679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">eng_Latn</th>\n",
              "      <th>asm_Beng</th>\n",
              "      <td>2960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ben_Beng</th>\n",
              "      <td>5715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brx_Deva</th>\n",
              "      <td>5317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    src_text\n",
              "src_lang dest_lang          \n",
              "asm_Beng eng_Latn       4036\n",
              "ben_Beng eng_Latn       1281\n",
              "brx_Deva eng_Latn       1679\n",
              "eng_Latn asm_Beng       2960\n",
              "         ben_Beng       5715\n",
              "         brx_Deva       5317"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trans_df.groupby([\"src_lang\", \"dest_lang\"]).agg({\"src_text\": \"count\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5SWEvRG6qfM",
        "outputId": "5f97d14a-7396-4436-e3b6-143f74a51632"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((6996, 6), (20988, 5))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.shape, trans_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVshkH_P6qfM",
        "outputId": "881ddf74-f197-45cb-aeaa-6e1a9ad79f12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HOF    3860\n",
              "NOT    3136\n",
              "Name: task_1, dtype: int64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.task_1.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLxGc3EJ6qfM"
      },
      "outputs": [],
      "source": [
        "dct = {\"data_df\": data_df, \"trans_df\": trans_df}\n",
        "pickle.dump(dct, open(\"./all_traslations.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZvmcCDx6qfM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rkzs0qx6qfN"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnrycVXd6qfN"
      },
      "outputs": [],
      "source": [
        "# !pip install tweetnlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHanhibW6qfN"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udrMyn_u6qfN"
      },
      "outputs": [],
      "source": [
        "trans_df[\"translated\"] = True\n",
        "data_df[\"translated\"] = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wHVQJYL6qfN",
        "outputId": "cc361562-ff20-4260-8519-78e3f236ea5a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src_text</th>\n",
              "      <th>translated_text</th>\n",
              "      <th>src_lang</th>\n",
              "      <th>dest_lang</th>\n",
              "      <th>label</th>\n",
              "      <th>translated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>তবে শুনলাম মমতা ব্যানার্জি কোটা পদ্ধতি তুলে দি...</td>\n",
              "      <td>But I have heard that Mamata Banerjee has remo...</td>\n",
              "      <td>ben_Beng</td>\n",
              "      <td>eng_Latn</td>\n",
              "      <td>NOT</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>প্রত্যেক দলের প্রত্যেক মন্ত্রী এমএলএ, এমপি, এদ...</td>\n",
              "      <td>Every minister of every party, MLA, MP, they h...</td>\n",
              "      <td>ben_Beng</td>\n",
              "      <td>eng_Latn</td>\n",
              "      <td>NOT</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>বিশ্ব জুড়ে বুদ্ধিজীবিদের সঙ্গে সম্পর্ক থাকে ব...</td>\n",
              "      <td>Universities, seminaries, publishing houses, a...</td>\n",
              "      <td>ben_Beng</td>\n",
              "      <td>eng_Latn</td>\n",
              "      <td>HOF</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            src_text  \\\n",
              "0  তবে শুনলাম মমতা ব্যানার্জি কোটা পদ্ধতি তুলে দি...   \n",
              "1  প্রত্যেক দলের প্রত্যেক মন্ত্রী এমএলএ, এমপি, এদ...   \n",
              "2  বিশ্ব জুড়ে বুদ্ধিজীবিদের সঙ্গে সম্পর্ক থাকে ব...   \n",
              "\n",
              "                                     translated_text  src_lang dest_lang  \\\n",
              "0  But I have heard that Mamata Banerjee has remo...  ben_Beng  eng_Latn   \n",
              "1  Every minister of every party, MLA, MP, they h...  ben_Beng  eng_Latn   \n",
              "2  Universities, seminaries, publishing houses, a...  ben_Beng  eng_Latn   \n",
              "\n",
              "  label  translated  \n",
              "0   NOT        True  \n",
              "1   NOT        True  \n",
              "2   HOF        True  "
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trans_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGkcOTUT6qfO",
        "outputId": "bd830e0f-3e91-43aa-f7e9-8057236abc6a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S. No.</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>split</th>\n",
              "      <th>lang</th>\n",
              "      <th>cleansed_truncated_text</th>\n",
              "      <th>translated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>তবে শুনলাম মমতা ব্যানার্জি কোটা পদ্ধতি তুলে দি...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>train</td>\n",
              "      <td>BE</td>\n",
              "      <td>তবে শুনলাম মমতা ব্যানার্জি কোটা পদ্ধতি তুলে দি...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   S. No.                                               text task_1  split  \\\n",
              "0       1  তবে শুনলাম মমতা ব্যানার্জি কোটা পদ্ধতি তুলে দি...    NOT  train   \n",
              "\n",
              "  lang                            cleansed_truncated_text  translated  \n",
              "0   BE  তবে শুনলাম মমতা ব্যানার্জি কোটা পদ্ধতি তুলে দি...       False  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11DP9okm6qfO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn import metrics\n",
        "\n",
        "# Training function: Performs forward propagation, backpropagation & optimization.\n",
        "# We also implement gradient clipping, which prevents the gradients from exploding\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, clip=1.0, v1=True):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    ep_t_loss = 0\n",
        "    batch_num  = 0\n",
        "    pred, tgt = [], []\n",
        "    for batch in tqdm(dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        optimizer.zero_grad()\n",
        "        if v1:\n",
        "            x_in_sam, x_attn_sam, y = batch\n",
        "            output = model(x_in_sam, x_attn_sam)\n",
        "        else:\n",
        "            x_in_sam, x_attn_sam, x_in_deva, x_attn_deva, y = batch\n",
        "            output = model(x_in_sam, x_attn_sam, x_in_deva, x_attn_deva)\n",
        "\n",
        "        loss = criterion(output.view(-1), y)\n",
        "        loss.backward()\n",
        "\n",
        "        #gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        ep_t_loss += loss.item()\n",
        "        batch_num += 1\n",
        "        pred.extend((torch.sigmoid(output.view(-1)) >= 0.5).tolist())\n",
        "        tgt.extend(y.tolist())\n",
        "\n",
        "    return ep_t_loss/batch_num, metrics.f1_score(tgt, pred, average='macro')\n",
        "\n",
        "def evaluate(model, dataloader, criterion, v1=True):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    ep_t_loss = 0\n",
        "    batch_num  = 0\n",
        "    pred, tgt = [], []\n",
        "    for batch in tqdm(dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            if v1:\n",
        "                x_in_sam, x_attn_sam, y = batch\n",
        "                output = model(x_in_sam, x_attn_sam)\n",
        "            else:\n",
        "                x_in_sam, x_attn_sam, x_in_deva, x_attn_deva, y = batch\n",
        "                output = model(x_in_sam, x_attn_sam, x_in_deva, x_attn_deva)\n",
        "\n",
        "            loss = criterion(output.view(-1), y)\n",
        "\n",
        "            ep_t_loss += loss.item()\n",
        "            batch_num += 1\n",
        "            pred.extend((torch.sigmoid(output.view(-1)) >= 0.5).tolist())\n",
        "            tgt.extend(y.tolist())\n",
        "\n",
        "    return ep_t_loss/batch_num, metrics.f1_score(tgt, pred, average='macro'), pred, tgt\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, base1, base2=None):\n",
        "        super().__init__()\n",
        "        self.base1 = base1\n",
        "        self.o1_drop = nn.Dropout(0.1)\n",
        "        if base2 is not None:\n",
        "            self.base2 = base2\n",
        "            self.o2_drop = nn.Dropout(0.1)\n",
        "        else:\n",
        "            self.base2 = None\n",
        "\n",
        "        if base2 is not None:\n",
        "            self.linear = nn.Linear(base1.config.hidden_size * 2, 1)\n",
        "        else:\n",
        "            self.linear = nn.Linear(base1.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, in_sam, attn_sam, in_deva=None, attn_deva=None):\n",
        "        o1 = self.o1_drop(self.base1(input_ids=in_sam, attention_mask=attn_sam)[\"pooler_output\"])\n",
        "        if self.base2 is not None and in_deva is not None:\n",
        "            o2 = self.o2_drop(self.base2(input_ids=in_deva, attention_mask=attn_deva)[\"pooler_output\"])\n",
        "            o = torch.cat([o1, o2], -1)\n",
        "            return self.linear(o)\n",
        "        else:\n",
        "            return self.linear(o1)\n",
        "\n",
        "def get_lang_dataset(lang_abbr):\n",
        "    t1 = trans_df[trans_df[\"dest_lang\"] == lang_map[lang_abbr][\"src\"]][[\"translated_text\", \"translated\", \"label\"]]\n",
        "    t2 = data_df[data_df[\"lang\"] == lang_abbr][[\"cleansed_truncated_text\", \"translated\", \"task_1\"]]\n",
        "    t1.columns = [\"text\", \"translated\", \"label\"]\n",
        "    t2.columns = [\"text\", \"translated\", \"label\"]\n",
        "    t = pd.concat([t1, t2])\n",
        "    t[\"lang\"] = lang_abbr\n",
        "    t = t.drop_duplicates().reset_index(drop=True)\n",
        "    return t\n",
        "\n",
        "def get_desc(row):\n",
        "    if row[\"lang\"] == \"A\":\n",
        "        if row[\"translated\"]:\n",
        "            return \"assamese translated:\"\n",
        "        else:\n",
        "            return \"assamese original:\"\n",
        "    if row[\"lang\"] == \"BE\":\n",
        "        if row[\"translated\"]:\n",
        "            return \"bengali translated:\"\n",
        "        else:\n",
        "            return \"bengali original:\"\n",
        "    if row[\"lang\"] == \"BO\":\n",
        "        if row[\"translated\"]:\n",
        "            return \"bodo translated:\"\n",
        "        else:\n",
        "            return \"bodo original:\"\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-6HH-CI6qfO",
        "outputId": "46b5f9c6-51f2-420f-c435-063647f3a206"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['S. No.', 'text', 'task_1', 'split', 'lang', 'cleansed_truncated_text',\n",
              "       'translated'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3tmVD3q6qfO",
        "outputId": "c803c977-c373-4fd4-acf5-904ec9645cbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6970, 4) (6952, 4) (6953, 4) (6952, 4)\n",
            "(6452, 5) (500, 5)\n"
          ]
        }
      ],
      "source": [
        "asm_df = get_lang_dataset(\"A\")\n",
        "ben_df = get_lang_dataset(\"BE\")\n",
        "bdo_df = get_lang_dataset(\"BO\")\n",
        "train_df = ben_df#pd.concat([asm_df, ben_df, bdo_df]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "print(asm_df.shape, ben_df.shape, bdo_df.shape, train_df.shape)\n",
        "\n",
        "train_df[\"label\"] = train_df.label.apply(lambda x : 1 if x == \"HOF\" else 0)\n",
        "train_df[\"description\"] = train_df.apply(get_desc, 1)\n",
        "train_df = train_df.sample(frac=1.0)\n",
        "vl_ = 500\n",
        "val_df = train_df[:vl_].sample(frac=1.0).reset_index(drop=True)\n",
        "train_df = train_df[vl_:].sample(frac=1.0).reset_index(drop=True)\n",
        "print(train_df.shape, val_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjusn3wq6qfO"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "nb_model_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjZ4MCN06qfO",
        "outputId": "10ed92ba-7e3c-406a-db08-8aa53dfb437b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOR  BE :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.80      0.73        65\n",
            "           1       0.59      0.43      0.50        44\n",
            "\n",
            "    accuracy                           0.65       109\n",
            "   macro avg       0.63      0.62      0.62       109\n",
            "weighted avg       0.64      0.65      0.64       109\n",
            " =============== \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for lng in ['BE']:#['A', 'BE', 'BO']:\n",
        "    tr_df = train_df[(train_df.lang == lng) & (~train_df.translated)].reset_index(drop=True)\n",
        "    vl_df = val_df[(val_df.lang == lng) & (~val_df.translated)].reset_index(drop=True)\n",
        "\n",
        "    tr_X, vl_X = list(tr_df.text), list(vl_df.text)\n",
        "    tr_Y, vl_Y = list(tr_df.label), list(vl_df.label)\n",
        "\n",
        "    tf_vec = CountVectorizer()\n",
        "    X_tr_tf = tf_vec.fit_transform(tr_X)\n",
        "    X_vl_tf = tf_vec.transform(vl_X)\n",
        "\n",
        "    nb_ = MultinomialNB()\n",
        "    nb_.fit(X_tr_tf, tr_Y)\n",
        "    vl_pred = nb_.predict(X_vl_tf)\n",
        "    print(\"FOR \",lng,\":\\n\",metrics.classification_report(vl_Y, vl_pred),\"===\"*5,\"\\n\")\n",
        "    nb_model_dict[lng] = {\"tf_vec\": tf_vec, \"nb_\": nb_}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZFdxVAz6qfO"
      },
      "outputs": [],
      "source": [
        "run = \"baseline_beng\"#\"deva\"#\"baseline\"#\"sam_deva\"#\"baseline\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMZ10yJlMaWP",
        "outputId": "107efd39-d6dc-4769-fa9c-10d0821c5378"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ai4bharat/IndicBERTv2-MLM-Sam-TLM were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "only sam input model\n",
            "The model has 278,042,113 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "tokenizer_sam = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBERTv2-MLM-Sam-TLM\")\n",
        "model_sam = AutoModel.from_pretrained(\"ai4bharat/IndicBERTv2-MLM-Sam-TLM\")\n",
        "model_deva = None\n",
        "\n",
        "if \"deva\" in run:\n",
        "    tokenizer_deva = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBERTv2-SS\")\n",
        "    model_deva = AutoModel.from_pretrained(\"ai4bharat/IndicBERTv2-SS\")\n",
        "\n",
        "if \"deva\" in run and \"sam\" in run:\n",
        "    print(\"Multi input model\")\n",
        "    classifier = Classifier(model_sam, model_deva)\n",
        "elif \"sam\" in run:\n",
        "    print(\"only sam input model\")\n",
        "    classifier = Classifier(model_sam)\n",
        "elif \"deva\" in run:\n",
        "    print(\"only deva input model\")\n",
        "    classifier = Classifier(model_deva)\n",
        "else:\n",
        "    print(\"only sam input model\")\n",
        "    classifier = Classifier(model_sam)\n",
        "\n",
        "print(f'The model has {count_parameters(classifier):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv0P9lXe6qfO"
      },
      "outputs": [],
      "source": [
        "train_sam_toks = tokenizer_sam(list(train_df[\"description\"] + train_df[\"text\"]), max_length=50, truncation=True,\n",
        "                               return_tensors=\"pt\", padding=True)\n",
        "val_sam_toks = tokenizer_sam(list(val_df[\"description\"] + val_df[\"text\"]), max_length=50, truncation=True,\n",
        "                             return_tensors=\"pt\", padding=True)\n",
        "if \"deva\" in run:\n",
        "    train_deva_toks = tokenizer_deva(list(train_df[\"description\"] + train_df[\"text\"]), max_length=50, truncation=True,\n",
        "                                     return_tensors=\"pt\", padding=True)\n",
        "    val_deva_toks = tokenizer_deva(list(val_df[\"description\"] + val_df[\"text\"]), max_length=50, truncation=True,\n",
        "                                   return_tensors=\"pt\", padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mReHpiB-6qfO",
        "outputId": "0bb2f448-b2d9-4da1-bb2a-04fbf6457c64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([6452, 50]), torch.Size([500, 50]))"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_sam_toks[\"input_ids\"].shape, val_sam_toks[\"input_ids\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IMKY_Y-6qfP",
        "outputId": "92a4fff8-e23d-4364-f52c-4573b0e89f55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([6452]), torch.Size([500]))"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_train, Y_val = torch.tensor(train_df[\"label\"])*1.0, torch.tensor(val_df[\"label\"])*1.0\n",
        "Y_train.shape, Y_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GseXBXQ76qfP",
        "outputId": "7615b3db-fca0-4361-9a19-66329686c48e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "only sam input dataloader\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Below we define a function to create train, test & valid dataloaders in Pytorch\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "def get_dataloader(input_ids, attention_mask, y, train=True):\n",
        "    data = TensorDataset(input_ids, attention_mask, y)\n",
        "    sampler = RandomSampler(data) if train else SequentialSampler(data)\n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "def get_dataloader_v2(sam_input_ids, sam_attention_mask,\n",
        "                      deva_input_ids, deva_attention_mask, y, train=True):\n",
        "    data = TensorDataset(sam_input_ids, sam_attention_mask,\n",
        "                         deva_input_ids, deva_attention_mask, y)\n",
        "    sampler = RandomSampler(data) if train else SequentialSampler(data)\n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "# train_dataloader = get_dataloader(train_sam_toks[\"input_ids\"], train_sam_toks[\"attention_mask\"], Y_train)\n",
        "# val_dataloader = get_dataloader(val_sam_toks[\"input_ids\"], val_sam_toks[\"attention_mask\"], Y_val)\n",
        "\n",
        "# train_dataloader = get_dataloader_v2(train_sam_toks[\"input_ids\"], train_sam_toks[\"attention_mask\"],\n",
        "#                                      train_deva_toks[\"input_ids\"], train_deva_toks[\"attention_mask\"], Y_train)\n",
        "# val_dataloader = get_dataloader_v2(val_sam_toks[\"input_ids\"], val_sam_toks[\"attention_mask\"],\n",
        "#                                    val_deva_toks[\"input_ids\"], val_deva_toks[\"attention_mask\"], Y_val)\n",
        "\n",
        "if \"deva\" in run and \"sam\" in run:\n",
        "    print(\"Multi input dataloader\")\n",
        "    train_dataloader = get_dataloader_v2(train_sam_toks[\"input_ids\"], train_sam_toks[\"attention_mask\"],\n",
        "                                     train_deva_toks[\"input_ids\"], train_deva_toks[\"attention_mask\"], Y_train)\n",
        "    val_dataloader = get_dataloader_v2(val_sam_toks[\"input_ids\"], val_sam_toks[\"attention_mask\"],\n",
        "                                       val_deva_toks[\"input_ids\"], val_deva_toks[\"attention_mask\"], Y_val)\n",
        "elif \"sam\" in run:\n",
        "    print(\"only sam input dataloader\")\n",
        "    train_dataloader = get_dataloader(train_sam_toks[\"input_ids\"], train_sam_toks[\"attention_mask\"], Y_train)\n",
        "    val_dataloader = get_dataloader(val_sam_toks[\"input_ids\"], val_sam_toks[\"attention_mask\"], Y_val)\n",
        "elif \"deva\" in run:\n",
        "    print(\"only deva input dataloader\")\n",
        "    train_dataloader = get_dataloader(train_deva_toks[\"input_ids\"], train_deva_toks[\"attention_mask\"], Y_train)\n",
        "    val_dataloader = get_dataloader(val_deva_toks[\"input_ids\"], val_deva_toks[\"attention_mask\"], Y_val)\n",
        "else:\n",
        "    print(\"only sam input dataloader\")\n",
        "    train_dataloader = get_dataloader(train_sam_toks[\"input_ids\"], train_sam_toks[\"attention_mask\"], Y_train)\n",
        "    val_dataloader = get_dataloader(val_sam_toks[\"input_ids\"], val_sam_toks[\"attention_mask\"], Y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWEGDvTA6qfP"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optim = torch.optim.AdamW(classifier.parameters(), lr = 2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFjhQb_A6qfP"
      },
      "outputs": [],
      "source": [
        "best_valid_loss = float('inf')\n",
        "tot_t_loss, tot_v_loss =[],[]\n",
        "N_EPOCHS = 20\n",
        "device = \"cuda:3\"\n",
        "early_stopping_marker = []\n",
        "early_stopping = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejScrTFP6qfP",
        "outputId": "3ec9e27b-db5f-46e6-8bd1-c12026bade46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mname: baseline_beng_model_least_loss_08192023_BE.pt | v1: True\n"
          ]
        }
      ],
      "source": [
        "classifier = classifier.to(device)\n",
        "mname = \"model_least_loss_08192023_BE.pt\" if run == \"baseline\" else run+\"_model_least_loss_08192023_BE.pt\"\n",
        "v1 = False if \"deva\" in run and \"sam\" in run else True\n",
        "print(\"mname:\", mname, \"| v1:\", v1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRx1q0OL6qfP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in tqdm(range(N_EPOCHS)):\n",
        "\n",
        "    tr_l, tr_f1= train(classifier, train_dataloader, optim, criterion, v1=v1)\n",
        "    tot_t_loss.append(tr_l)\n",
        "\n",
        "    val_l, val_f1, pred, tgt = evaluate(classifier, val_dataloader, criterion, v1=v1)\n",
        "    tot_v_loss.append(val_l)\n",
        "\n",
        "    if val_l <= best_valid_loss:\n",
        "        best_valid_loss = val_l\n",
        "        best_pred, best_tgt = pred, tgt\n",
        "        torch.save(classifier.state_dict(), mname)\n",
        "        print(\"\\nBest Model Saved !!\")\n",
        "        early_stopping_marker.append(False)\n",
        "    else:\n",
        "        early_stopping_marker.append(True)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Total Loss: {tr_l:.3f} | Train F1: {tr_f1:.3f}')\n",
        "    print(f'\\tVal. Total Loss: {val_l:.3f} | Valid F1: {val_f1:.3f}')\n",
        "    print(\"_________________________________________________________________\")\n",
        "    if all(early_stopping_marker[-early_stopping:]) and len(early_stopping_marker) >= early_stopping:\n",
        "        print(\"Early stopping training as the Val loss did NOT improve for the last \"+str(early_stopping)+\" epochs.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZVJx2Bl6qfP",
        "outputId": "0793ad26-86c5-4080-b3ec-dc6e4bfeb5ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "baseline_beng_model_least_loss_08192023_BE.pt Model Loaded!\n"
          ]
        }
      ],
      "source": [
        "classifier.load_state_dict(torch.load(mname))\n",
        "classifier.eval()\n",
        "print(mname,\"Model Loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlryUDEP6qfQ",
        "outputId": "2604260d-f9d4-4e77-91be-d863e84efeb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'baseline'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run = \"sam_deva\"#\"baseline\"\n",
        "run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHusOogX6qfQ",
        "outputId": "711f3450-b8e7-4831-c355-c17fb783af1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.17308353, 0.85997318, 0.07644555, ..., 0.76766401, 0.6153633 ,\n",
              "       0.58595888])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nb_pred_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHBAP6QH6qfQ"
      },
      "outputs": [],
      "source": [
        "pth = \"~/hasoc_2023_ub/datasets/task4_annihilate/\"\n",
        "version = \"4-deva-\" #\"\"\n",
        "for i in os.listdir(pth):\n",
        "    if \"test\" in i and \"chetona_pred\" not in i and i.split(\"_\")[1] == \"BE\":\n",
        "        print(\"Running predictions for\",i)\n",
        "        tdf_ = pd.read_csv(pth+i)\n",
        "        tdf_[\"lang\"], tdf_[\"translated\"] = i.split(\"_\")[1], False\n",
        "        tdf_[\"description\"] = tdf_.apply(get_desc, 1)\n",
        "\n",
        "#         sam_toks = tokenizer_sam(list(tdf_[\"description\"] + tdf_[\"text\"]), max_length=50, truncation=True,\n",
        "#                              return_tensors=\"pt\", padding=True)\n",
        "        if \"deva\" in run and \"sam\" in run:\n",
        "            sam_toks = tokenizer_sam(list(tdf_[\"description\"] + tdf_[\"text\"]), max_length=50, truncation=True,\n",
        "                             return_tensors=\"pt\", padding=True)\n",
        "            deva_toks = tokenizer_deva(list(tdf_[\"description\"] + tdf_[\"text\"]), max_length=50, truncation=True,\n",
        "                                     return_tensors=\"pt\", padding=True)\n",
        "            data_ = TensorDataset(sam_toks[\"input_ids\"], sam_toks[\"attention_mask\"],\n",
        "                                 deva_toks[\"input_ids\"], deva_toks[\"attention_mask\"])\n",
        "        elif \"sam\" in run:\n",
        "            sam_toks = tokenizer_sam(list(tdf_[\"description\"] + tdf_[\"text\"]), max_length=50, truncation=True,\n",
        "                             return_tensors=\"pt\", padding=True)\n",
        "            deva_toks = None\n",
        "            data_ = TensorDataset(sam_toks[\"input_ids\"], sam_toks[\"attention_mask\"])\n",
        "        elif \"deva\" in run:\n",
        "            deva_toks = tokenizer_deva(list(tdf_[\"description\"] + tdf_[\"text\"]), max_length=50, truncation=True,\n",
        "                             return_tensors=\"pt\", padding=True)\n",
        "            sam_toks = None\n",
        "            data_ = TensorDataset(deva_toks[\"input_ids\"], deva_toks[\"attention_mask\"])\n",
        "        else:\n",
        "            sam_toks = tokenizer_sam(list(tdf_[\"description\"] + tdf_[\"text\"]), max_length=50, truncation=True,\n",
        "                             return_tensors=\"pt\", padding=True)\n",
        "            deva_toks = None\n",
        "            data_ = TensorDataset(sam_toks[\"input_ids\"], sam_toks[\"attention_mask\"])\n",
        "\n",
        "        dataloader_ = DataLoader(data_, sampler=SequentialSampler(data_), batch_size=batch_size)\n",
        "\n",
        "        \"\"\" Make predictions \"\"\"\n",
        "        nb_pred_ = nb_model_dict[tdf_.lang.iloc[0]][\"nb_\"].predict_proba(\\\n",
        "                            nb_model_dict[tdf_.lang.iloc[0]][\"tf_vec\"].transform(list(tdf_.text)))[:,-1]\n",
        "        pred_ = []\n",
        "        for batch_ in tqdm(dataloader_):\n",
        "            batch_ = tuple(t.to(device) for t in batch_)\n",
        "            if \"deva\" in run and \"sam\" in run:\n",
        "                sam_in_, sam_attn_, deva_in_, deva_attn_ = batch_\n",
        "                with torch.no_grad():\n",
        "                    output_ = classifier(in_sam=sam_in_, attn_sam=sam_attn_,\n",
        "                                         in_deva=deva_in_, attn_deva=deva_attn_)\n",
        "            else:\n",
        "                sam_in_, sam_attn_ = batch_\n",
        "                deva_in_, deva_attn_ = None, None\n",
        "                with torch.no_grad():\n",
        "                    output_ = classifier(in_sam=sam_in_, attn_sam=sam_attn_,\n",
        "                                         in_deva=deva_in_, attn_deva=deva_attn_)\n",
        "            pred_.extend((torch.sigmoid(output_.view(-1))).tolist())\n",
        "\n",
        "        pred_ = (np.asarray(pred_)>= 0.5).tolist()#(((0.8*np.asarray(pred_) + 0.2*nb_pred_)) >= 0.5).tolist()\n",
        "        fop_ = pth+i.replace(\".csv\", \"_\"+run+\"_ver_\"+version+\"_chetona_pred.csv\")\n",
        "        tdf_[\"pred\"] = [\"HOF\" if i else \"NOT\" for i in pred_]\n",
        "        tdf_[\"pred_nb\"] = [\"HOF\" if i else \"NOT\" for i in nb_pred_]\n",
        "        tdf__ = tdf_[['S. No.', 'pred']]\n",
        "        tdf__.columns = [\"S. No.\", \"task_1\"]\n",
        "        print(tdf__.shape, \"Predictions done for:\", i)\n",
        "        print(\"Output location:\", fop_)\n",
        "\n",
        "        tdf__.to_csv(fop_, index=False)\n",
        "        # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0MQpM4j6qfS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}